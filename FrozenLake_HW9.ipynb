{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfe/Tk8V4z4SWOj1dz9okI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msilaev/ML-PatternRecCourse/blob/main/FrozenLake_HW9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This is a homework assignment in ML course. The task is to find manually an optimal Q table for the Frozen Lake game in the slippery case. There is a good tutorial on this topic https://youtu.be/Vrro7W7iW2w"
      ],
      "metadata": {
        "id": "o0OWCaO1iYA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "rAPNbPD9iVoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_policy(qtable_, num_of_episodes_, max_steps_):\n",
        "    rewards = []\n",
        "\n",
        "    for episode in range(num_of_episodes_): # This is out loop over num of episodes\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "\n",
        "        for step in range(max_steps_):\n",
        "            action = np.argmax(qtable_[state,:])\n",
        "            new_state, reward, done, truncated = env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "            else:\n",
        "                state = new_state\n",
        "        rewards.append(total_reward)\n",
        "        env.close()\n",
        "    return sum(rewards)/num_of_episodes_"
      ],
      "metadata": {
        "id": "MQDVm0FcePN4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=True, render_mode=\"ansi\")\n",
        "\n",
        "action_size = env.action_space.n\n",
        "print(\"Action size: \", action_size)\n",
        "\n",
        "state_size = env.observation_space.n\n",
        "print(\"State size: \", state_size)\n"
      ],
      "metadata": {
        "id": "wyYkf4BoeTeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In general it can be seen that less \"dangerous\" way is 0-> 4-> 8 -> 9 -> 13 -> 14 -> 15.  We dont't want to go from 0 to 1, therefore we should command to go from 0 to the left. Let's check the probabilities of actions in this case. Below the command is env.P[state][action], where action = 0,1,2,3 corresponding to \"left\", \"down\", \"right\", \"up\"."
      ],
      "metadata": {
        "id": "dxkrQmZFZexv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.P[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZ52H2u-cJwC",
        "outputId": "ce519dd6-060c-4218-ec40-d5854575b016"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.3333333333333333, 0, 0.0, False),\n",
              " (0.3333333333333333, 0, 0.0, False),\n",
              " (0.3333333333333333, 4, 0.0, False)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We see that agent either stays in 0 or moves to 4. The same trick can be done with other states to avoid drawning. The resulting Q table is below. One can check that the sucess probability is almost the same as with the brute force Monte Carlo solution."
      ],
      "metadata": {
        "id": "-toVEcBddl3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qtable_1 = np.zeros([state_size, action_size])\n",
        "# start(0) -> go left (0)\n",
        "qtable_1[0,0] = 1\n",
        "# state(4) -> go left (0)\n",
        "qtable_1[4,0] = 1\n",
        "# state(8) -> go right (2)\n",
        "qtable_1[8,2] = 1\n",
        "# state(9) -> go down (1)\n",
        "qtable_1[9,1] = 1\n",
        "# state(13) -> go right (2)\n",
        "qtable_1[13,2] = 1\n",
        "# state(10) -> go left (0)\n",
        "qtable_1[10,0] = 1\n",
        "\n",
        "# state 1 -> go up (3)\n",
        "qtable_1[1,3] = 1\n",
        "\n",
        "# state 1 -> go left (0)\n",
        "#qtable[1,0] = 1\n",
        "\n",
        "# state 2 -> go up (3)\n",
        "qtable_1[2,3] = 1\n",
        "# state 3 -> go up (3)\n",
        "qtable_1[3,3] = 1\n",
        "# state 6 -> go down (1)\n",
        "qtable_1[6,1] = 1\n",
        "# state 10 -> go left (1)\n",
        "qtable_1[10,0] = 1\n",
        "# state 14 -> go down (2) -> GOAL or 13\n",
        "qtable_1[14,1] = 1"
      ],
      "metadata": {
        "id": "kZ0Cfr4DVUvC"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(qtable)\n",
        "num_of_episodes_ = 10000\n",
        "print(f'Total reward for manually and \"smartly\" defined Q-table: {eval_policy(qtable_1, num_of_episodes_, 100)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIrBa6YeYpmF",
        "outputId": "ed7d2b98-0fa9-4400-cd72-d5b269306467"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total reward for manually and \"smartly\" defined Q-table: 0.2161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5b2oavTEe3VJ"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}